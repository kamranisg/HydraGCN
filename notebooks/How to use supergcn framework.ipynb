{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Pull one of our docker images\n",
    "# docker pull gerome_v/...\n",
    "\n",
    "# Run docker container using pulled docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gitlab repo (supergcn_2.0 repo feature/batchload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: git: not found\n",
      "/bin/sh: 1: git: not found\n"
     ]
    }
   ],
   "source": [
    "# !git checkout feature/batchload\n",
    "# !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hydra components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataset\n",
    "    - Dataset setup\n",
    "        - save and specify which file paths to load.\n",
    "    - EdgeCriterion\n",
    "        - criterion for initial graph construction.\n",
    "   \n",
    "2. Model\n",
    "    - torch.nn.Module\n",
    "3. Trainer\n",
    "    - transductive training ('full' mode) or inductive training ('batch' mode).\n",
    "4. Config\n",
    "    - global dictionary containing information about all the classes and configuration from user defined yaml file\n",
    "\n",
    "** Basically, you can also jump to 4. Config if there is no need to add custom components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataset setup\n",
    "- Inherit from `DatasetSetupBuilder` so that Hydra will know this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./supergcn_2.0/base/datasets/dataset_setup.py\n",
    "\n",
    "# When setting up dataset. It always inherits from DatasetSetupBuilder\n",
    "class MyDataset(DatasetSetupBuilder):\n",
    "    def __init__(self):\n",
    "        super(MyDataset, self).__init__()\n",
    "\n",
    "    def build_dataset(self):\n",
    "        # This method is called in order to build the datasets\n",
    "        # You can write code here to build then save the numpy dataset\n",
    "        # within the filesystem\n",
    "        pass\n",
    "    \n",
    "    # This dictionary will be used internally when loading the dataset.\n",
    "    # You just have to specify where the files are located.\n",
    "    self.dirs = {\n",
    "                 'img': [folder_dir],\n",
    "                 'img_paths': [general_dir + 'MNIST_numpy/img_paths.npy'],\n",
    "                 'feat': [general_dir + 'MNIST_numpy/features.npy'],\n",
    "                 'seq': [general_dir + 'MNIST_numpy/seq_data.npy'],\n",
    "        \n",
    "                 # Define targets here\n",
    "                 'targets': [general_dir + 'MNIST_numpy/targets.npy'],\n",
    "                 \n",
    "                 # Loss function\n",
    "                 'loss': [{'loss': 'CrossEntropyLoss', 'wts': 1.0}],\n",
    "                 'meta': general_dir + 'MNIST_numpy/meta_data.npy',\n",
    "                 'adj': general_dir + 'MNIST_numpy',\n",
    "                 \n",
    "                 # The 0th column in the meta data matrix will be used as adjacency\n",
    "                 'adjtoinput': {'img': [0],\n",
    "                                'feat': [0],\n",
    "                                'seq': [0]}\n",
    "                     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `./base/configs/default.yaml` \n",
    "# Specify which columns in your meta data you want to use\n",
    "# This will be used when constructing the initial graph\n",
    "# Below we are using 0th, 1st, and 2nd column of the meta data\n",
    "meta_columns:\n",
    "    - 0 \n",
    "    - 1\n",
    "    - 2\n",
    "\n",
    "# `./base/datasets/dataset_base.py`\n",
    "# Define similarity metric using EdgeCriterionBuilder\n",
    "class MyDatasetEdgeCriterion(EdgeCriterionBuilder):\n",
    "    def __init__(self):\n",
    "        super(MyDatasetEdgeCriterion, self).__init__()\n",
    "    \n",
    "    # Method needed to return the adjacency matrix as numpy array (N x N) \n",
    "    # where N are the number of nodes.\n",
    "    @staticmethod\n",
    "    def edge_criterion(meta_data, meta_col):        \n",
    "        # Thresholds you want to use for every meta information\n",
    "        threshold = [2, 0, 5]  # Dummy dataset age, gender, weight\n",
    "        \n",
    "        # Define whichever similarity metric you want for as long as \n",
    "        # you return an N x N numpy array.\n",
    "        dist = np.abs(meta_data[:, meta_col] - meta_data[:, meta_col, None])\n",
    "        edges = dist <= threshold[meta_col]\n",
    "        return edges.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model\n",
    "- inherit from `ModelBuilder`\n",
    "- Model configuration can be specified within the yaml file\n",
    "- `forward` method's input is a dictionary and should also return a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example GCN model\n",
    "class MyModel(ModelBuilder):\n",
    "    def __init__(self, conf, idx):\n",
    "        super(MyModel, self).__init__()\n",
    "        gnn_list = []\n",
    "        for layer in range(conf.layers):\n",
    "            kwargs = self.set_kwargs(conf, layer, idx)\n",
    "            gnn_list.append(tg.GCNConv(**kwargs))\n",
    "        self.model_list = nn.ModuleList(gnn_list)\n",
    "        self.activation = Config.global_dict['model'][conf.activation.name]\n",
    "        self.activation = self.activation(**conf.activation)\n",
    "\n",
    "    def forward(self, x_dict):\n",
    "        x, adj, adj_wts = x_dict['input'], x_dict['adj'], x_dict.get('adj_wts', None)\n",
    "        for model in self.model_list:\n",
    "            x = model(x, adj, adj_wts)\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        # Make sure we return all objects within the dict\n",
    "        out_dict = {k: v for k, v in x_dict.items()}\n",
    "        \n",
    "        # Include new output\n",
    "        out_dict['input'] = x\n",
    "        out_dict['adj'] = adj\n",
    "        return out_dict\n",
    "\n",
    "\n",
    "\n",
    "# Example yaml configuration how to use the module above\n",
    "# We always need a DynamicBlock and AggregatorBlock pairs\n",
    "MyModel:\n",
    "    # -----------------------------------------\n",
    "    # Pair 0\n",
    "    DynamicBlock0:\n",
    "        # the distributor is a list of indices specifying which input you want to use\n",
    "        # From above example of MyDataset, Hydra will yield a \"batch\" of images, a feature vector, \n",
    "        # and a sequential input.\n",
    "        # [{'input': image_tensor}, {'input': feat_tensor}, {'input': sequential_tensor}]\n",
    "        \n",
    "        # The distributor will distribute which input tensor will be use.\n",
    "        # You can leave this blank as `distributor:` and the system will automatically generate a list [0, 1, 2].\n",
    "        distributor:\n",
    "            - 0        \n",
    "        ParallelGNN:\n",
    "            Model0:\n",
    "                model: MyModel\n",
    "                    \n",
    "                # Which input from the distributor will be used\n",
    "                order:\n",
    "                    - 0\n",
    "                layers: 2\n",
    "                activation:\n",
    "                    name: LeakyReLU\n",
    "                    negative_slope: 0.2\n",
    "\n",
    "                layer0:\n",
    "                    in_channels:\n",
    "                    out_channels: 16\n",
    "                    K: 3\n",
    "\n",
    "                layer1:\n",
    "                    in_channels: 16\n",
    "                    out_channels:\n",
    "                    K: 3\n",
    "\n",
    "    AggregatorBlock0:\n",
    "        # list of list of indices to allow aggregation of multiple\n",
    "        # input from the previous block\n",
    "        distributor:\n",
    "            -\n",
    "                - 0\n",
    "        ParallelAgg:\n",
    "            Agg0:\n",
    "                \n",
    "                # You can use whichever aggregator from `base.models.aggregator.py`\n",
    "                # Here we are not doing anything to the inputs and we are just passing\n",
    "                # them to the next block.\n",
    "                agg: Pass\n",
    "                order:\n",
    "                    - 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Trainer\n",
    "- Handles training in full and batch mode.\n",
    "- `KFoldModelTrainer` performs k-fold stratified cross-validation.\n",
    "- just specify in `default.yaml` file\n",
    "\n",
    "    - batchtype: 'full' # for transductive learning\n",
    "\n",
    "    - batchtype: 'batch' # for inductive learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in case you want to create a custom training workflow just inherit from TrainerBuilder (`base.trainer.train_base.TrainerBuilder`) and specify in defaults.yaml that you want to use this instead of our default `KFoldModelTrainer`.\n",
    "\n",
    "\n",
    "`train:\n",
    "      name: MyNewKFoldModelTrainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Config\n",
    "- given that you want to use one of the datasets within Hydra and that the model components within Hydra are already enough for your experiments including the default trainer.\n",
    "- you can just setup your configuration using a yaml file\n",
    "    - here there are two important yaml files\n",
    "       1. default.yaml\n",
    "       2. model_architecture.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the default.yaml\n",
    "# Folder location of user-defined classes\n",
    "# You can ignore this for now\n",
    "include: /workspace/supergcn_2.0/setups/\n",
    "\n",
    "# Dataset class names\n",
    "dataset:\n",
    "  name: UKBBDataset # Dataset\n",
    "  datatype: BatchDataset # We use the same class for full and batch mode. Ignore for now.\n",
    "  edge_criterion: UKBBMEdgeCriterion # used to initialize the graph {MyDataset}EdgeCriterion\n",
    "  p_missing: 0.0 # [0.0, 1.0) level of missingness, 0.0 means use all available data\n",
    "  preprocess_feat: true # Set to True to standardized data (zero-mean and unit variance scaling)\n",
    "\n",
    "# Model class names\n",
    "model:\n",
    "  name: DynamicNet  # This is the general builder class for all models no need to change this\n",
    "  yaml_path:  '../base/configs/models/HydraGNN.yaml' # location of your model architecture\n",
    "\n",
    "# User-defined transformation for image inputs\n",
    "user_transforms:\n",
    "  - RandomRotation:\n",
    "      degrees: 1\n",
    "\n",
    "# Trainer class name\n",
    "train:\n",
    "  name: KFoldModelTrainer # stratified k-fold cross-validation trainer\n",
    "  tensorboard: false # whether to use tensorboard or not\n",
    "\n",
    "#\n",
    "infer:\n",
    "  name:\n",
    "\n",
    "# Arguments currently in use.\n",
    "batchtype: 'full' #\n",
    "folds: 10 # number of cross-validation folds\n",
    "train_mode: classification\n",
    "\n",
    "# Column index of meta data to use\n",
    "meta_columns:\n",
    "  - 0\n",
    "  - 1\n",
    "  - 2\n",
    "multigraph: true\n",
    "epochs: 600\n",
    "patience: 30\n",
    "supervision_rate: 1.0\n",
    "num_print: 10\n",
    "no_cuda: false\n",
    "GPU_device: '0'\n",
    "writing: false\n",
    "seed: 42\n",
    "batch_size: 10\n",
    "label_list:\n",
    "  - 0\n",
    "  - 1\n",
    "  - 2\n",
    "lr: 0.001\n",
    "weight_decay: 0.005\n",
    "dropout: 0.3\n",
    "alpha: 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
